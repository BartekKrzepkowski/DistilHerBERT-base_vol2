{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470860f-7bb2-4137-88c5-8a7c3440c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bd21b-3cd1-4c42-b2be-2ec809c00433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# init accelerator\n",
    "# accelerator = Accelerator(fp16=True)\n",
    "# device = accelerator.device\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 32 // BATCH_SIZE\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c5ad0-a6c4-410e-accd-fa33d2da5bef",
   "metadata": {},
   "source": [
    "## Get datasets: alternative - Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b071a-be4a-42b7-807e-0ae119224d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.herbert.tokenization_herbert_fast import HerbertTokenizerFast\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "raw_datasets = load_dataset(\"allegro/klej-nkjp-ner\")\n",
    "target_mapper = {label: i for i, label in enumerate(raw_datasets['train'].unique('target'), 0)}\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(example['sentence'], truncation=True)\n",
    "    tokenized['labels'] = [target_mapper[target] for target in example['target']] \n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence', 'token_type_ids', 'target'])\n",
    "tokenized_datasets = tokenized_datasets.with_format('torch')\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def get_dataloaders(tokenizer, tokenized_datasets):\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_set = tokenized_datasets['train']\n",
    "    test_set = tokenized_datasets['validation']\n",
    "    train = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    test = DataLoader(dataset=test_set, shuffle=False, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(tokenizer, tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35985e-9eb5-4461-a6c7-3970a6dcb9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "polemo_herbert = AutoModelForSequenceClassification.from_pretrained(\"allegro/herbert-base-cased\", num_labels=len(target_mapper))\n",
    "# polemo_herbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eef306-c25c-44ee-8405-78cdf18b2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set accelerator\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from trainers.utils import configure_optimizer\n",
    "\n",
    "optim = configure_optimizer(polemo_herbert.bert, polemo_herbert.classifier, AdamW,\n",
    "                            lr_backbone=5e-5, lr_head=5e-4, weight_decay=1e-3)\n",
    "\n",
    "# TU ZMIENIŁEM\n",
    "# train_loader, test_loader, polemo_model, optim = accelerator.prepare(\n",
    "#     train_loader, test_loader, polemo_model, optim)\n",
    "\n",
    "loaders  = {'train': train_loader, 'test': test_loader}\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# scheduler = CosineAnnealingLR(optim, len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS, 0)\n",
    "\n",
    "NUM_TRAINING_STEPS = len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optim,\n",
    "        num_cycles=EPOCHS,\n",
    "        num_warmup_steps=int(0.5 * NUM_TRAINING_STEPS),\n",
    "        num_training_steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ffa9f-42e6-4f95-863a-773bad07b207",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# polemo_herbert.bert.requires_grad_(False)\n",
    "# polemo_herbert.bert.pooler.requires_grad_(True)\n",
    "\n",
    "# for name, params in polemo_herbert.named_parameters():\n",
    "#     if params.requires_grad and ('encoder' in name or 'embeddings' in name):\n",
    "#         params.requires_grad = False\n",
    "#     print(name, params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb938c-da5a-4722-93d1-1eddc0deb1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.vanillaTrainerClassifier import VanillaTrainerClassifier\n",
    "\n",
    "# TU ZMIENIŁEM\n",
    "params_trainer = {\n",
    "    'model': polemo_herbert.to(device),\n",
    "    'tokenizer': tokenizer,\n",
    "    'loaders': loaders,\n",
    "    'criterion': nn.CrossEntropyLoss().to(device),\n",
    "    'optim': optim,\n",
    "    'scheduler': scheduler,\n",
    "    # 'accelerator': accelerator,\n",
    "    'device': device\n",
    "}\n",
    "trainer = VanillaTrainerClassifier(**params_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27a1cb-3c29-47d4-8692-cc2241ce8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490a14-def6-4df0-bbee-535fd4f6a82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "config_run_epoch = collections.namedtuple('RE', ['save_interval', 'grad_accum_steps', 'running_step'])(20, GRAD_ACCUM_STEPS, 40)\n",
    "\n",
    "# TU ZMIENIŁEM\n",
    "params_run = {\n",
    "    'epoch_start': 0,\n",
    "    'epoch_end': EPOCHS,\n",
    "    'exp_name': f'herbert_ner_replication, hf_dataset, backbone classifier split, dynamically_freeze_layers, step:2',\n",
    "    'config_run_epoch': config_run_epoch,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "trainer.run_exp(**params_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249e2fe-f313-41a3-b889-a21f86549fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.n_logger.run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a638e-2355-418b-9ab4-597fe229e516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
