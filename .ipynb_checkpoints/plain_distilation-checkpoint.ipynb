{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5470860f-7bb2-4137-88c5-8a7c3440c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9bd21b-3cd1-4c42-b2be-2ec809c00433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# init accelerator\n",
    "# accelerator = Accelerator(device_placement=True, fp16=True, mixed_precision='fp16')\n",
    "# device = accelerator.device\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 1024 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a2536c-1ca7-4655-9aac-cb121425b6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.sso.sso_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.sso.sso_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: Teacher: 124442880, Student: 81323520,Student / Teacher ratio: 0.6535.\n"
     ]
    }
   ],
   "source": [
    "from trainers.utils import get_teacher_student_tokenizer, print_model_size\n",
    "teacher, student, tokenizer = get_teacher_student_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613a1e00-458e-4e73-89da-0bb94c4e321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497.85 MB\n",
      "325.34 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(teacher)\n",
    "print_model_size(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f09c57a-b561-4f43-ada7-f4ec6dce58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "def get_dataloaders(tokenizer, path_tokenized_dataset):\n",
    "    tokenized_datasets = load_from_disk(path_tokenized_dataset)\n",
    "    train_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "    test_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=False)\n",
    "    train_set = tokenized_datasets['train']\n",
    "    test_set = tokenized_datasets['test']\n",
    "    train = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collator)\n",
    "    test = DataLoader(dataset=test_set, shuffle=False, batch_size=BATCH_SIZE, collate_fn=test_collator)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(tokenizer, 'datasets/tokenized_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75eef306-c25c-44ee-8405-78cdf18b2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set accelerator\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from trainers.utils import configure_optimizer\n",
    "\n",
    "optim = configure_optimizer(AdamW, student, None, lr_backbone=1e-4, lr_head=None, weight_decay=1e-3)\n",
    "\n",
    "# train_loader, test_loader, teacher, student, optim = accelerator.prepare(\n",
    "#     train_loader, test_loader, teacher, student, optim)\n",
    "\n",
    "loaders  = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "NUM_TRAINING_STEPS = len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optim,\n",
    "        num_cycles=EPOCHS,\n",
    "        num_warmup_steps=int(0.01 * NUM_TRAINING_STEPS),\n",
    "        num_training_steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bfb938c-da5a-4722-93d1-1eddc0deb1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.distilTrainer_hf_collator import DistilTrainer\n",
    "\n",
    "params_trainer = {\n",
    "    'teacher': teacher.to(device),\n",
    "    'student': student.to(device),\n",
    "    'tokenizer': tokenizer,\n",
    "    'loaders': loaders,\n",
    "    'criterion1': nn.CrossEntropyLoss().to(device),\n",
    "    'criterion2': nn.CrossEntropyLoss().to(device),\n",
    "    # 'criterion2': nn.KLDivLoss('batchmean').to(device), # mam używać log_target?\n",
    "    'criterion3': nn.CosineEmbeddingLoss().to(device),\n",
    "    'optim': optim,\n",
    "    'scheduler': scheduler,\n",
    "    # 'accelerator': accelerator,\n",
    "    'device': device\n",
    "}\n",
    "trainer = DistilTrainer(**params_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c27a1cb-3c29-47d4-8692-cc2241ce8ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-54d950c1ee6c766a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-54d950c1ee6c766a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490a14-def6-4df0-bbee-535fd4f6a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/rm360179/DistilHerBERT/e/DIS-490\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73cf9ec8a0a40c88d747a9f8a77511c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_exp:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87a48a84b5d402aa21b6d7b9bd23319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_epoch: train:   0%|          | 0/283001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartekkrzepkowski/.virtualenvs/tldl/lib/python3.8/site-packages/transformers/data/data_collator.py:892: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers.Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "config_run_epoch = collections.namedtuple('RE', ['save_interval', 'grad_accum_steps', 'running_step'])(20, GRAD_ACCUM_STEPS, 30)\n",
    "\n",
    "params_run = {\n",
    "    'epoch_start': 0,\n",
    "    'epoch_end': EPOCHS,\n",
    "    'exp_name': f'plain_distil_hf_collator,preprocessing,deduplication,whole_word_masking_hf',\n",
    "    'config_run_epoch': config_run_epoch,\n",
    "    'temp': 2.0,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "trainer.run_exp(**params_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249e2fe-f313-41a3-b889-a21f86549fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.n_logger.run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0955caf-e6ad-4315-98e2-d9d92e2750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e17f9-36ad-4916-bb48-01c4dc509a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf52352-4b2c-4ae6-abfb-e543091118ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
