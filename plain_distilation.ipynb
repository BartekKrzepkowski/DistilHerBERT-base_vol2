{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470860f-7bb2-4137-88c5-8a7c3440c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bd21b-3cd1-4c42-b2be-2ec809c00433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# init accelerator\n",
    "# accelerator = Accelerator(device_placement=True, fp16=True, mixed_precision='fp16')\n",
    "# device = accelerator.device\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 1024 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2536c-1ca7-4655-9aac-cb121425b6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.utils import get_teacher_student_tokenizer, print_model_size\n",
    "teacher, student, tokenizer = get_teacher_student_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a1e00-458e-4e73-89da-0bb94c4e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_size(teacher)\n",
    "print_model_size(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09c57a-b561-4f43-ada7-f4ec6dce58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "def get_dataloaders(tokenizer, path_tokenized_dataset):\n",
    "    tokenized_datasets = load_from_disk(path_tokenized_dataset)\n",
    "    train_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "    test_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=False)\n",
    "    train_set = tokenized_datasets['train']\n",
    "    test_set = tokenized_datasets['test']\n",
    "    train = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collator)\n",
    "    test = DataLoader(dataset=test_set, shuffle=False, batch_size=BATCH_SIZE, collate_fn=test_collator)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(tokenizer, 'data/tokenized_dataset_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05c593-940d-4848-8f1f-70a70cc42cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set accelerator\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from trainers.utils import configure_optimizer\n",
    "\n",
    "optim = configure_optimizer(AdamW, student, None, lr_backbone=5e-5, lr_head=None, weight_decay=1e-3)\n",
    "\n",
    "# train_loader, test_loader, teacher, student, optim = accelerator.prepare(\n",
    "#     train_loader, test_loader, teacher, student, optim)\n",
    "\n",
    "loaders  = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "NUM_TRAINING_STEPS = len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optim,\n",
    "        num_cycles=EPOCHS,\n",
    "        num_warmup_steps=int(0.01 * NUM_TRAINING_STEPS),\n",
    "        num_training_steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb938c-da5a-4722-93d1-1eddc0deb1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.distilTrainer_hf_collator import DistilTrainer\n",
    "\n",
    "params_trainer = {\n",
    "    'teacher': teacher.to(device),\n",
    "    'student': student.to(device),\n",
    "    'tokenizer': tokenizer,\n",
    "    'loaders': loaders,\n",
    "    'criterion1': nn.CrossEntropyLoss().to(device),\n",
    "    'criterion2': nn.CrossEntropyLoss().to(device),\n",
    "    # 'criterion2': nn.KLDivLoss('batchmean').to(device), # mam używać log_target?\n",
    "    'criterion3': nn.CosineEmbeddingLoss().to(device),\n",
    "    'optim': optim,\n",
    "    'scheduler': scheduler,\n",
    "    # 'accelerator': accelerator,\n",
    "    'device': device\n",
    "}\n",
    "trainer = DistilTrainer(**params_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27a1cb-3c29-47d4-8692-cc2241ce8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490a14-def6-4df0-bbee-535fd4f6a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "config_run_epoch = collections.namedtuple('RE', ['save_interval', 'grad_accum_steps', 'running_step'])(20, GRAD_ACCUM_STEPS, 30)\n",
    "\n",
    "params_run = {\n",
    "    'epoch_start': 0,\n",
    "    'epoch_end': EPOCHS,\n",
    "    'exp_name': f'plain_distil_hf_collator,preprocessing,deduplication,whole_word_masking_hf',\n",
    "    'config_run_epoch': config_run_epoch,\n",
    "    'temp': 2.0,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "trainer.run_exp(**params_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249e2fe-f313-41a3-b889-a21f86549fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.n_logger.run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0955caf-e6ad-4315-98e2-d9d92e2750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e17f9-36ad-4916-bb48-01c4dc509a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf52352-4b2c-4ae6-abfb-e543091118ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
