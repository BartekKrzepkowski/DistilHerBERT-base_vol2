{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3582a52c-cc3e-4a42-8673-c49db5efc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7904fb-d94f-4ce9-a4bd-ef0b40728db3",
   "metadata": {},
   "source": [
    "# Create Smaller Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141b3c7-c417-4ed1-bb01-e1ec4342d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cc100/pl_5e7.txt', 'w') as f:\n",
    "    for i, line in enumerate(open('data/cc100/pl.txt', 'r')):\n",
    "        if i >= 5e7:\n",
    "            break\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c228ab0-b235-41b2-a204-85dafbd5cb5c",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37701237-192b-41ad-b658-abc8055d790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0350b0-e1e1-4baa-aefa-3ca9e7d9c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IN = 'data/cc100/pl_5e7.txt'\n",
    "PATH_OUT = 'data/cc100_filtered_5e7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930da26-441a-46fc-912c-d5ffe15284d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2d789698c2e0870d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/bartekk/.cache/huggingface/datasets/text/default-2d789698c2e0870d/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14371e6965e41c189bb2c647cdfca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f97f626c464653b7d98f6f0eb6e28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/bartekk/.cache/huggingface/datasets/text/default-2d789698c2e0870d/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b3cb2c3ee8443d823039c1bad5f645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_dataset.<locals>.<lambda> at 0x7f0388cca320> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e88294a5164811ad659a2cf8327e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde98f4f704d4f8fa4bcf0c58bb0599f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fd5a6917e44c0ea81647b98ba9e933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d3d6a132884c6d83458a22f8232b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4435dbbaeefe4a88a2237aaac60ff4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a22a1a4380b48699e66a5cf9acc1c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86499e25b4c4062ad85649701b62c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1460c33a9c1e4d9cae68081e7a15b8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/6250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_dataset(path_in, path_out):\n",
    "    raw_datasets = load_dataset('text', data_files=path_in)\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    \n",
    "    import re\n",
    "    import html as ihtml\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = BeautifulSoup(ihtml.unescape(text), \"lxml\").text\n",
    "        text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    filter_non_alfanum = lambda x: re.sub('[^0-9AaĄąBbCcĆćDdEeĘęFfGgHhIiJjKkLlŁłMmNnŃńOoÓóPpRrSsŚśTtUuWwYyZzŹźŻż\\,\\. ]+', '', x)\n",
    "    filter_ratio = lambda x: len(filter_non_alfanum(x)) / len(x)\n",
    "    \n",
    "    raw_datasets = raw_datasets.filter(lambda x: len(x['text']) > 15, num_proc=NUM_PROC)\n",
    "    raw_datasets = raw_datasets.map(lambda x: {'text':  [clean_text(y) for y in x['text']]}, batched=True, num_proc=NUM_PROC)\n",
    "    raw_datasets = raw_datasets.filter(lambda x: len(x['text']) > 15 and filter_ratio(x['text']) > 0.9, num_proc=NUM_PROC)\n",
    "    raw_datasets.save_to_disk(path_out)\n",
    "    \n",
    "preprocess_dataset(PATH_IN, PATH_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63263ee5-2117-4bb3-8acd-aa6bc11f9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dedup_datasets = load_from_disk(PATH_OUT)\n",
    "dedup_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f571b7-12f8-4e2d-88f5-237f8bcbe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_datasets.shuffle()['train'].select(range(5))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1874ed7-c0f1-41b9-9c72-a7f665f732fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.herbert.tokenization_herbert_fast import HerbertTokenizerFast\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6f73c-6197-41ef-bd5c-24d548f9ae15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#after_deduplication\n",
    "import glob\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "dedup_datasets = [load_dataset('json', data_files=path)['train'] for path in glob.glob('./datasets/data/*.json.gz')]\n",
    "dedup_dataset = concatenate_datasets(dedup_datasets)\n",
    "dedup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70da11-b75f-40ed-b09f-8513452d3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver1\n",
    "def tokenize_dataset1(dedup_dataset, path_tokenized_out):\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    def tokenize_function(example):\n",
    "        tokenized = tokenizer(example['text'], truncation=True)\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dedup_dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['text', 'token_type_ids'])\n",
    "    tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "    tokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.01, seed=29)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.save_to_disk(path_tokenized_out)\n",
    "    \n",
    "tokenize_dataset1(dedup_datasets, 'data/tokenized_dataset_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a13585-6c27-4d68-8d51-b91409ab89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ver2\n",
    "def get_proper_idx1(idx, context_length, words_ids):\n",
    "    if idx + context_length >= len(words_ids) - 1:\n",
    "        return idx + context_length, idx + context_length\n",
    "    if words_ids[idx + context_length] != words_ids[idx + context_length - 1]:\n",
    "        return idx + context_length, idx + context_length\n",
    "    else:\n",
    "        while words_ids[idx + context_length] == words_ids[idx + context_length - 1]:\n",
    "            idx -= 1\n",
    "        return idx + context_length, idx + context_length\n",
    "            \n",
    "def get_proper_idx2(idx, context_length, words_ids):\n",
    "    if idx + context_length >= len(words_ids) - 1:\n",
    "        return idx + context_length, idx + context_length\n",
    "    if words_ids[idx + context_length - 1] == None:\n",
    "        return idx + context_length, idx + context_length\n",
    "    else:\n",
    "        while words_ids[idx + context_length] == words_ids[idx + context_length - 1]:\n",
    "            idx -= 1\n",
    "        lidx = idx\n",
    "        ridx = idx\n",
    "        while words_ids[lidx + context_length - 1] != None:\n",
    "            lidx -= 1\n",
    "        while words_ids[ridx + context_length - 1] != None:\n",
    "            ridx += 1\n",
    "        lidx = lidx + context_length\n",
    "        ridx = ridx + context_length\n",
    "        idx = idx + context_length\n",
    "        \n",
    "        if idx - lidx < 20:\n",
    "            return lidx, lidx\n",
    "        elif ridx - idx < 20:\n",
    "            return idx, ridx\n",
    "        else:\n",
    "            return idx, idx\n",
    "            \n",
    "\n",
    "def tokenize_dataset2(dedup_dataset, path_tokenized_out, context_length=400):\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    # nie dodawaj tokenów specjalnych\n",
    "    def tokenize_function(example):\n",
    "        all_input_ids = [0]\n",
    "        all_words_ids = [None]\n",
    "        results = tokenizer(example['text'], add_special_tokens=False)\n",
    "        for i, input_ids in enumerate(results['input_ids']):\n",
    "            all_input_ids.extend(input_ids)\n",
    "            all_input_ids.append(tokenizer.sep_token_id)\n",
    "            \n",
    "            all_words_ids.extend(results.word_ids(i))\n",
    "            all_words_ids.append(None)\n",
    "        chunks1 = []\n",
    "        chunks2 = []\n",
    "        i = 0\n",
    "        while i < len(all_input_ids):\n",
    "            j_min, j_max = get_proper_idx2(i, context_length, all_words_ids)\n",
    "            # problem z ucinaniem słow\n",
    "            chunks1.append([0] + all_input_ids[i: j_min])\n",
    "            chunks2.append([None] + all_words_ids[i: j_min])\n",
    "            i = j_max\n",
    "        return {'input_ids': chunks1, 'word_ids': chunks2}\n",
    "\n",
    "    tokenized_dataset = dedup_dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC, remove_columns=['text'])\n",
    "    # tokenized_dataset = tokenized_dataset.remove_columns(['text', 'token_type_ids'])\n",
    "    # tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x['input_ids']) >= 30, num_proc=NUM_PROC)\n",
    "    tokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.01, seed=29)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.save_to_disk(path_tokenized_out)\n",
    "    \n",
    "tokenize_dataset2(dedup_datasets, 'data/tokenized_dataset_demo2', context_length=tokenizer.model_max_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd779efd-1694-46a1-951a-cc75297447a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from models.collator import DataCollatorForWholeWordMask\n",
    "BATCH_SIZE = 8\n",
    "def get_dataloaders(tokenizer, path_tokenized_dataset):\n",
    "    tokenized_datasets = load_from_disk(path_tokenized_dataset)\n",
    "    train_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "    test_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "    train_set = tokenized_datasets['train']\n",
    "    test_set = tokenized_datasets['test']\n",
    "    train = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collator)\n",
    "    test = DataLoader(dataset=test_set, shuffle=False, batch_size=BATCH_SIZE, collate_fn=test_collator)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(tokenizer, 'data/tokenized_dataset_demo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89da5ce-49c5-4b69-8e05-1dfafd8bf228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c3d36-8542-4a00-b6dd-f423d45cdfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk('data/tokenized_dataset_demo2')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a9b99-eb2b-44ac-9c9d-f23d6011f395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][4]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27395b1-baac-4555-8daa-1e392f8b37c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5065d-089e-4b10-a209-d4ca1d9e5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a1a4-d778-48ca-99ca-1268ce10691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675ae1c-c85d-42c6-9557-fcdcf5445116",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<div>\n",
    "<h1>Title</h1>\n",
    "<p>A long text........ </p>\n",
    "<a href=\"\"> a link </a>\n",
    "</div>\"\"\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72de8c1-d5b1-4e06-940c-f679bacae0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.unescape(html.escape(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85c9fc-c2a2-4048-9cc2-b2a80766d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46efa5-9c21-4108-8e33-1e4ae42fc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(html.unescape(text), \"lxml\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18612546-b6fd-43f8-9190-6e78b220abb5",
   "metadata": {},
   "source": [
    "# Dataset Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e71035-f4d0-4eb8-99a6-501f379ade50",
   "metadata": {},
   "source": [
    "### Num of Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce655816-4d7e-4363-acd8-b6939e030148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a76f44-963c-446b-a410-7ad56f014cbd",
   "metadata": {},
   "source": [
    "### Num of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1545cac-0ead-480b-8190-99f688e64135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0bc98-9905-4726-a628-6b7bc5898e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ss = tokenized_datasets['train'].map(lambda x: {'#token': len(x['input_ids'])})\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(ss['#token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c1e4b-3888-4ae0-83cd-c4d537abe3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = ss.filter(lambda x: len(x['input_ids']) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b9fb3-99da-404e-8a75-fa0581b9a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[nb for nb in dd['#token'] if nb < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf806bd2-0a75-4421-a7bf-596a44143a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcfce2-d7c0-42d8-8e7a-e3ae56e18cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f269ba4-32e9-41d9-a4e6-2788ccb9ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import DummyScheduler, DummyOptim\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1585526-8b93-4db5-9cc9-fdd49d76dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = DummyOptim(Linear(2,3).parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b304630-f9a8-4e71-b45c-64fe4e6c3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DummyScheduler(optim, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11387bc-60f5-4b5c-bf75-8df5b3f50bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189d760-0901-46f4-a2de-03e3acf5b8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
